

\section{E2017120501: Data Partitioning on Fragmented XML Data}

This report is about the experiments to evaluate the performance
of applying data partitioning strategy on fragmented XML data.

\subsection{A Brief Introduction to Implementation}

The implementation can now process some of the queries on fragmented
XML documents with data partitioning in a distributed-memory environment.
There are still problems existed to be solved, which cause some 
queries unable to be successfully evaluated.

\subsection{Settings}

The settings of this experiments are listed below.

\paragraph{Hardware}

There were 1 master (HaoDesk) and 16 workers (matsu-lab00 -- matsu-lab15)
used in the experiments listed in Appendix~\ref{app:hardware}.
The master and workers were in the same local network at speed of 1 Gbps.
 


\paragraph{Software}
The implementation is written in Java (64-Bit Server VM version 1.8.0\_151).
The input XML data set was xmark600(see \ref{app:xqueries}), which was
fragmented into 16 fragments with maxisze=4M.
On each worker, there ran a BaseX server (Verion: 8.6.7).
For each fragment, a BaseX database instance, namely `xmark600\_16\_4M\'
was created in main memory on each worker. To understand the memory
consumption, the database size of \texttt{xmark600\_16\_4M} of each workers
are listed in Table~\ref{tab:dbinfo}. The input size in the table
is the size of input XML document, while database size is the size
of the database created in BaseX, including all indexes and related data.
For the data partition, the number of partions(P) were  1, 2, 3, 4, 8.
The intermediate database for holding the results of prefix
query were named  `xmark600\_16\_4M\_tmp'. Since all workers equip with
8 GB memory, we can create database in main memory mode.


\begin{table}
	\centering
	\label{tab:dbinfo}
	\caption{Information of database sizes on workers.}
	\begin{tabular}{c|c|c}
		\hline
		worker &  input size & database size \\
		\hline
		matsu-lab00 & 3897 MB & 4293 MB \\
		matsu-lab01 & 3613 MB & 3807 MB \\
		matsu-lab02 & 3662 MB & 3788 MB \\
		matsu-lab03 & 4001 MB & 4434 MB \\
		matsu-lab04 & 3750 MB & 4112 MB \\
		matsu-lab05 & 3907 MB & 4255 MB \\
		matsu-lab06 & 4369 MB & 5021 MB \\
		matsu-lab07 & 4524 MB & 5226 MB \\
		matsu-lab08 & 3605 MB & 3876 MB \\
		matsu-lab09 & 4159 MB & 4611 MB \\
		matsu-lab10 & 3571 MB & 3720 MB \\
		matsu-lab11 & 4005 MB & 4434 MB \\
		matsu-lab12 & 4333 MB & 4995 MB \\
		matsu-lab13 & 4309 MB & 4864 MB \\
		matsu-lab14 & 4219 MB & 4743 MB \\
		matsu-lab15 & 4621 MB & 5363 MB \\
		\hline
	\end{tabular}
\end{table}




\paragraph{XQuery Expressions}

Two XPath expressions XQ1 and XQ2 for prefix and suffix queries
are listed in Section~\ref{app:xqueries}.



\subsection{Confirming Correctness}

The number of hit nodes, the order of hit nodes and results size of
the final output of queries have been checked and confirmed to be correct.
All the successfully evaluated queries
have the same number of hit nodes in original order. Some may be
different in size, such as xm3a.dps. Its original result size
was 922,270,281, but in this experiment, it is 883,253,777.
This dramatical difference is caused by line-break. The previous
experiments were done on Windows, while the current on Linux.
Since BaseX using `$\backslash$r$\backslash$n' on Windows, while
`$\backslash$n' on Linux for line-break, there is one byte
difference for each new line. We also found that the query has
6,502,751 hit nodes and  there are six lines in every hit node,
such as:\\

\begin{table}
	\centering
	\label{code:bidder}
	\caption{A hit node of xm3.dps}
	\footnotesize
	\begin{lstlisting}[language=java,frame=single]
	<bidder>
	<date>08/04/1999</date>
	<time>11:15:36</time>
	<personref person="person17793"/>
	<increase>7.50</increase>
	</bidder>
	\end{lstlisting}
\end{table}



We then have 883,253,777 + 6,502,751 * 6 - 922,270,281 = 2. These two bytes are
extra '$\backslash$n'. So the sizes are the same.



\subsection{Discussion on Queries}

The execution time of queries and related information are listed in
Table~\ref{tab:exetime}.

\subsection{Timing}

There were three period of times recorded, T$_p$, T$_s$ and T$_m$. T$_p$ is the
time for executing prefix query. It starts from sending  prefix query to all
workers and ends with the complete of prefix query on all workers.
T$_s$ is the time for executing suffix query, starting from sending till all
workers done. T$_m$ is the time for merging results.
For comparison, T$_o$ that represents the execution time of xm1.org -- xm6.org 
is also listed in the table.

\paragraph{xm1.dps}

Since the result of xm1.dps is larger than the memory of HaoDesk, this query was
not evaluated. This is a design  choice about how to process the results. Based
the previous evaluation, one possible way is to stored the unordered
intermediate results in files and then concatenate these files.

\paragraph{xm2.dps}

This query was not tested for two reasons. Firstly, the prefix query of xm2.dps
(as well as xm4.dps) cannot be processed by the current XQuery expression XQ1
(will be modifed and tested soon). Secondly, there query has no hit node over
XMark600 as explained in experiment E20170401. To keep using this query, one
proposal is to make a minimal change to the attribute part of the query, such as
changing \texttt{category52} to \texttt{category324329}, which exists in
XMark600.


\paragraph{xm3.dps}

The execution times of suffix part increases with respect to the number of P.
Although the workers have only 4 cores, still we can get less execution time
with more partitions. 


\paragraph{xm4.dps}

Not tested for the same reason as the first one of xm2.dps.

\paragraph{xm5.dps}

The query xm5a.dps was the target query in the previous design.
However, due to the insufficient number of nodes in the results of prefix
query to be allocated to 16 computers\footnote{
	Detailed explanation: An error message
	``database 'xmark600\_16\_4M\_tmp' has no node with PRE value 5.'' was
	encountered when executing a suffix query on an intermediate database.
	The database had the content of  \texttt{<root><part> ...</part></root>} ,
	where there is only one \texttt{part} node.
	In a suffix query when P = 2, it is then to process ``pre value 5'' in XQ2,
	which refers to the second \texttt{part} node. However, since there is
	no second \texttt{part} node, the error occurred.
}, I changed it to xm5.dps (xm5b.dps was also evaluated, but
it took 1480s (P=4), which is too long and thus ignored).


\paragraph{xm6.dps}

Failed to evaluate it with data partitioning. The reason is
the same as xm5a.dps (both \texttt{/site/regions} and
\texttt{/site/regions/*[...]/item} were tested).

\input{experiments/results}


\subsection{Problems}

In general there are several problems (or design choices) listed below:

\paragraph{Evaluation cannot be done.}

This is the primary problem that hinder the implementation to process
all queries successfully.
The current implementation for some reason cannot successfully finish
the evaluation very often. It went dead many times in the middle of
evaluating suffix queries. For example, letting P=4, there are 64 suffix
queries processed in parallel. The evaluation stays irresponsible after
50 or more suffix queries have been evaluated. The reason has not been
figured out yet.

\paragraph{Memory size is not enough for xm1.dps.}
The result of xm1.dps is nearly 60 GB, which is larger than the memory
of HaoDesk (32 GB). One proposal is to keep the results on disk.


\paragraph{Performance is low.}
The evaluations for some suffix were very low in performance.
For example, 




\subsection{Efficiency of Parsing Intermediate Results of Suffix Query}

A important method that affects the performance of the implementation is
\texttt{basex.PreValueReceiver.process(InputStream input)} used
to parse the received results of suffix query, (i.e. PRE value +
content). It takes the results of suffix query and returns an
QueryResultsPre instance with a list of PRE values and a list of
string content (of the same size). A simple experiment were done to evaluate
the parsing speed. In the experiment, it took 465 ms to parse
52,757 KiB data with 704,430 nodes, i.e. it can process 100 MiB
data per second, which nearly reach the maximum network
speed and thus should be sufficient for not being a bottleneck.






%\begin{table}
%	\centering
%	\label{tab:xm5b.dps}
%	\caption{Execution time for xm5b.dps}
%	\begin{tabular}{c|c|c|c|c|c|c|c}
%		\hline
%		query    &  P=1  &  P=2   &  P=3  &  P=4 &   P=6 &    P=8 & P=12   \\
%		\hline
%		prefix    &   \multicolumn{6}{|c}{$\approx$ 0.3s}    \\
%		\hline
%		suffix    & 243s & 67.5s &  48.9s & 37.8s & 33.4s&   31.8s &   28.2s \\
%		\hline
%		merge     &   \multicolumn{6}{|c}{$\approx$ 10s}    \\
%		\hline
%	\end{tabular}
%\end{table}
%
%\begin{table}
%	\centering
%	\label{tab:xm3dps}
%	\caption{Execution time for xm3.dps}
%	\begin{tabular}{c|c|c|c|c|c|c|c}
%		\hline
%		query    &  P=1  &  P=2   &  P=3  &  P=4 &   P=6 &    P=8 & P=12   \\
%		\hline
%		prefix    &   \multicolumn{6}{|c}{$\approx$ 0.3s}    \\
%		\hline
%		suffix    & 113.2s&  58.5s &  30.1s&  20.2s & 15.8s&   13.5s & 11.3s \\
%		\hline
%		merge     &   \multicolumn{6}{|c}{$\approx$ 6s}    \\
%		\hline
%	\end{tabular}
%\end{table}

