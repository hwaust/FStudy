

\section{E2017120501: Data Partitioning on Fragmented XML Data}

This report is about the experiments to evaluate the performance 
of applying data partitioning strategy on fragmented XML data. 


\subsection{A Brief Introduction to Implementation}

The implementation can now process some of the queries on fragmented 
XML data with data partitioning in a distributed-memory environment.
There are still problems existed to be solved.


\subsection{Settings}

The settings of this experiments are listed below.

\paragraph{Hardware}

There were 1 master (HaoDesk) and 16 workers (matsu-lab00 -- matsu-lab15) 
used in the experiments listed in \ref{app:xqueries}.



\paragraph{Software}
The input XML data set was xmark600(see \ref{app:xqueries}), which was 
fragmented into 16 fragments with maxisze=4M.
On each worker, there ran a BaseX server (Verion: 8.6.7). 
For each fragment, a BaseX database instance, namely `xmark600\_16\_4M\'
was created in main memory on each worker.
For the data partition, the number of partions(P) were  1, 2, 3, 4, 8.
The intermediate database for holding the results of prefix
query were named  `xmark600\_16\_4M\_tmp'.
 

\paragraph{XQuery Expressions}

Two XPath expressions XQ1 and XQ2 for prefix and suffix queries 
are listed in Section~\ref{app:xqueries}.



\subsection{Confirming Correctness}

The number of hit nodes, the order of hit nodes and results size of
the final output of queries have been checked and confirmed to be correct. 
All the successfully evaluated queries 
have the same number of hit nodes in original order. Some may be 
different in size, such as xm3a.dps. Its original result size
was 922,270,281, but in this experiment, it is 883,253,777.
This dramatical difference is caused by line-break. The previous 
experiments were done on Windows, while the current on Linux. 
Since BaseX using `$\backslash$r$\backslash$n' on Windows, while
 `$\backslash$n' on Linux for line-break, there is one byte 
difference for each new line. We also found that the query has 
6,502,751 hit nodes and  there are six lines in every hit node, 
such as:\\
\verb|<bidder>|\\
\verb|  <date>08/04/1999</date>|\\
\verb|  <time>11:15:36</time>|\\
\verb|  <personref person="person17793"/>|\\
\verb|  <increase>7.50</increase>|\\
\verb|</bidder>|\\

We then have 883,253,777 + 6,502,751 * 6 - 922,270,281 = 2. These two bytes are
extra '$\backslash$n'. So the sizes are the same.



\subsection{Discussion on Queries}

The execution time of queries and related information are listed in
Table~\ref{tab:exetime}. 

\subsection{Timing}

There were three period of times recorded, T$_p$, T$_s$ and T$_m$. T$_p$ is the
time for executing prefix query. It starts from sending  prefix query to all
workers and ends with the complete of prefix query on all workers. 
T$_s$ is the time for executing suffix query, starting from sending till all 
workers done. T$_m$ is the time for merging results. 


\paragraph{xm1.dps}.

Since the result of xm1.dps is larger than the memory of HaoDesk, this query was
not evaluated. This is a design  choice about how to process the results. Based
the previous evaluation, one possible way is to stored the unordered
intermediate results in files and then concatenate these files.

\paragraph{xm2.dps}

This query was not tested for two reasons. Firstly, the prefix query of xm2.dps
(as well as xm4.dps) cannot be processed by the current XQuery expression XQ1
(will be modifed and tested soon). Secondly, there query has no hit node over
XMark600 as explained in experiment E20170401. To keep using this query, one
proposal is to make a minimal change to the attribute part of the query, such as
changing \texttt{category52} to \texttt{category324329}, which exists in
XMark600.


\paragraph{xm3.dps}





\paragraph{xm4.dps}

Not tested for the same reason as the first one of xm2.dps.

\paragraph{xm5.dps}

The query xm5a.dps was the targe query in the previous design. 
However, due to the insufficient number of nodes in the results of prefix 
query to be allocated to 16 
computers\footnote{
Detailed explanation: An error message 
``database 'xmark600\_16\_4M\_tmp' has no node with pre value 5.'' was
encountered when executing a suffix query on an intermediate database.
The database had the content of  \texttt{<root><part> ...</part></root>} , 
where there is only one \texttt{part} node. 
In a suffix query when P = 2, it is then to process ``pre value 5'' in XQ2, 
which refers to the second \texttt{part} node. However, since there is 
no second \texttt{part} node, the error occurred.
}
, I changed it to xm5b.dps (xm5c.dps was also evaluated, but 
it took 1480s (P=4), which is too long and thus ignored). 
The results of xm5b.dps is listed in Table~\ref{tab:xm5b.dps}
(xm5.dps = 75.05s). 


\paragraph{xm6.dps}

Failed to evaluate it with data partitioning. The reason is 
the same as xm5a.dps (both \texttt{/site/regions} and 
\texttt{/site/regions/*[...]/item} were tested).

\input{result}

 
\subsection{Problems}

In general there are several problems (or design choices) listed below:


\paragraph{Memory size is limited.} 
The result of xm1.dps is nearly 60 GB, which is larger than the memory 
of HaoDesk (32 GB). One proposal is to keep the results on disk. 

\paragraph{Evaluation cannot finish.} 

The implementation for some reason cannot finish the evaluation. It 
often went dead in the middle of evaluating suffix queries. For example, 
letting P=4, there are 64 suffix queries processed in parallel. 
The evaluation stays irresponsive after 50 or more suffix queries have 
been evaluated. The reason has not been figured out yet. 


\paragraph{Low Performance} 
The evaluations for some suffix were very low in performance.  
There are two examples to show that.
Firstly,  




\subsection{Efficiency of Parsing Intermediate Results of Suffix Query}

A important method in the implementation that affects the whole performance is 
\texttt{basex.PreValueReceiver.process(InputStream input)} used 
to parse the received results of suffix query, (i.e. PRE value + 
content). It takes the results of suffix query and returns an 
QueryResultsPre instance with a list of PRE values and a list of 
string content (of the same size). A simple experiment were done to evaluate 
the parsing speed. In the experiment, it took 465 ms to parse 
52,757 KiB data with 704,430 nodes, i.e. it can process 100 MiB 
data per second, which basically reach the maximum network 
speed and thus should be sufficient for not being a bottleneck.




%\begin{table}
%	\centering
%	\label{tab:xm5b.dps}
%	\caption{Execution time for xm5b.dps}
%	\begin{tabular}{c|c|c|c|c|c|c|c}
%		\hline
%		query    &  P=1  &  P=2   &  P=3  &  P=4 &   P=6 &    P=8 & P=12   \\
%		\hline
%		prefix    &   \multicolumn{6}{|c}{$\approx$ 0.3s}    \\
%		\hline
%		suffix    & 243s & 67.5s &  48.9s & 37.8s & 33.4s&   31.8s &   28.2s \\ 
%		\hline
%		merge     &   \multicolumn{6}{|c}{$\approx$ 10s}    \\
%		\hline
%	\end{tabular} 
%\end{table} 
%
%\begin{table}
%	\centering
%	\label{tab:xm3dps}
%	\caption{Execution time for xm3.dps}
%	\begin{tabular}{c|c|c|c|c|c|c|c}
%		\hline
%		query    &  P=1  &  P=2   &  P=3  &  P=4 &   P=6 &    P=8 & P=12   \\
%		\hline
%		prefix    &   \multicolumn{6}{|c}{$\approx$ 0.3s}    \\
%		\hline
%		suffix    & 113.2s&  58.5s &  30.1s&  20.2s & 15.8s&   13.5s & 11.3s \\ 
%		\hline
%		merge     &   \multicolumn{6}{|c}{$\approx$ 6s}    \\
%		\hline
%	\end{tabular} 
%\end{table} 

